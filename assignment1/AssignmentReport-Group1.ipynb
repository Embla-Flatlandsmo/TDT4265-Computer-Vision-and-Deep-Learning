{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "We have to access $w$, which is part $\\hat{y}$. So, first, we differentiate with respect to $\\hat{y}$:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial C}{\\partial \\hat{y}} &=\\frac{\\partial}{\\partial \\hat{y}}\\left(-\\left(y^{n} \\ln \\left(\\hat{y}^{n}\\right)+\\left(1-y^{n}\\right) \\ln \\left(1-\\hat{y}^{n}\\right)\\right)\\right.\\\\\n",
    "&=-\\left(\\frac{y^{n}}{\\hat{y}^{n}}-\\frac{\\left(1-y^{n}\\right)}{1-\\hat{y}^{n}}\\right)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Then, by substituting $\\frac{\\partial \\hat{y}^n}{\\partial w_i}$ with equation 7 from the assignment text, we have:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial C}{\\partial \\omega_{1}}=\\frac{\\partial C}{\\partial \\hat{y}^n} \\cdot \\frac{\\partial \\hat{y}^n}{\\partial w_{i}} &=-\\left(\\frac{y^{n}}{\\hat{y}^{n}}-\\frac{\\left(1-y^{n}\\right)}{\\left(1-\\hat{y}^{n}\\right)}\\right) x_{i}^n \\hat{y}^{n}\\left(1-\\hat{y}^{n}\\right) \\\\\n",
    "&=-\\left(y^{n}\\left(1-\\hat{y}^{n}\\right)-\\hat{y}^{n}\\left(1-y^{n}\\right)\\right) x_{i}^n \\\\\n",
    "&=-\\left(y^{n}-y^{n} \\hat{y}^{n}-\\hat{y}^{n}+\\hat{y}^{n} y^{n}\\right) x_{i}^n \\\\\n",
    "&=-\\left(y^{n}-\\hat{y}^{n}\\right) x_{i}^n\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "For ease of notation I have chosen to omit the n, meaning $y_k=y_k^n$ and $\\hat{y}_k=\\hat{y}_k^n$\n",
    "\n",
    "Do the chain rule to figure out which partial derivatives I must put together:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial C^n(w)}{\\partial w_{kj}} = \\frac{\\partial C^n(w)}{\\partial \\hat{y}_k} \\frac{\\partial \\hat{y}_k}{\\partial z_k} \\frac{\\partial z_k}{\\partial w_{kj}}\n",
    "\\end{equation}\n",
    "\n",
    "Let's do the simplest ones first. For $\\frac{\\partial z_k}{\\partial w_{kj}}$, because all $w$ where $i\\neq j = 0$ we get:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial z_k}{\\partial w_{kj}}=x_j\n",
    "\\end{equation}\n",
    "\n",
    "For $\\frac{\\partial C^n (w)}{\\partial \\hat{y}_k}$ we can move the partial derivative into the sum and compute it using the derivative rule $\\frac{d}{dx}log(x)=\\frac{1}{x}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial C^n (w)}{\\partial \\hat{y}_k} &= \\frac{\\partial}{\\partial \\hat{y}_k}(-\\sum_{k=1}^K y_k ln(\\hat{y}_k) \\\\\n",
    "    &= -\\sum_{k=1}^K y_k \\frac{\\partial }{\\partial \\hat{y}_k}ln(\\hat{y}_k) \\\\\n",
    "    &= -\\sum_{k=1}^K \\frac{y_k}{\\hat{y}_k}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "$\\frac{\\partial \\hat{y}_k}{\\partial z_k}$ is more complicated. We start out by simplifying using the log derivative rule:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\frac{d}{dx}ln(y) & =\\frac{1}{y} \\frac{dy}{dx} \\\\\n",
    "\\Rightarrow\\frac{dy}{dx} & =y \\frac{d}{dx}ln(y)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "I also perform some simplification on $ln(\\hat{y}_k)$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\ln \\left(\\hat{y}_{k}\\right) &=\\ln \\left(\\frac{e^{z_{k^\\prime}}}{\\sum_{k^{\\prime}}^{k} e^{z_{k}}}\\right) \\\\\n",
    "&=\\ln \\left(e^{z_{k}}\\right)-\\ln \\left(\\sum_{k^{\\prime}}^{k} e^{z_{k^{\\prime}}}\\right) \\\\\n",
    "&=z_{k} \\ln (e)-\\ln \\left(\\sum_{k^{\\prime}}^{k} e^{z_{k^{\\prime}}}\\right) \\\\\n",
    "&=z_{k}-\\ln \\left(\\sum_{k^{\\prime}}^{k} e^{z_{k^{\\prime}}}\\right)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "This leads to the following derivative for the index j:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial z_{j}}\\left(\\ln \\left(\\hat{y}_{k}\\right)\\right) &=\\frac{\\partial z_{k}}{\\partial z_{j}}-\\frac{\\partial}{\\partial z_{j}}\\left(\\ln \\left(\\sum_{k^{\\prime}}^{k} e^{z_{k^{\\prime}}}\\right)\\right) \\\\\n",
    "&=\\frac{\\partial z_{k}}{\\partial z_{j}}-\\frac{1}{\\sum_{k^{\\prime}}^{k} e^{z_{k^{\\prime}}}} \\cdot \\frac{\\partial}{\\partial z_{j}} \\sum_{k^{\\prime}}^{k} e^{z_{k^{\\prime}}} \\\\\n",
    "&=\\frac{\\partial z_{k}}{\\partial z_{j}}-\\frac{e^{z_{j}}}{\\sum_{k^{\\prime}}^{k} e^{z_{k^{\\prime}}}} \\\\\n",
    "&=\\frac{\\partial z_{k}}{\\partial z_{j}}-\\hat{y}_{j}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Where the simplification for the second to last line comes from that $\\frac{\\partial}{\\partial z_{j}} \\sum_{k^{\\prime}}^{k} e^{z_{k^{\\prime}}} = 0$ for all indices except for $k^\\prime=j$, where it is $e^{z_j}$.\n",
    "\n",
    "\n",
    "By inserting these values into the log derivative we get:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\hat{y}_k}{\\partial z_j}&=\\hat{y}_k\\frac{\\partial}{\\partial z_j}ln(\\hat{y}_k) \\\\\n",
    " &= \\hat{y}_k(\\frac{\\partial z_{k}}{\\partial z_{j}}-\\hat{y}_{j})\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "From here arises two situations: $j=k$ and $j\\neq k$.\n",
    "\n",
    "For $j=k$ we have $\\frac{\\partial z_k}{\\partial z_j}=1$:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\hat{y}_k }{\\partial z_k } = \\hat{y}_k (1-\\hat{y}_k)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "For $j\\neq k$ we have $\\frac{\\partial z_k}{\\partial z_j}=0$:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\hat{y}_k }{\\partial z_j } =-\\hat{y}_k\\hat{y}_j\n",
    "\\end{equation}\n",
    "\n",
    "We can then combine the results in the following expression for $\\frac{\\partial C^n(w)}{\\partial w_{kj}}$:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial C^n(w)}{\\partial w_{kj}} &=  \\left( \\left(-\\sum_{k=1, k \\neq j}^K \\frac{y_k}{\\hat{y}_k}(-\\hat{y}_k\\hat{y}_j) \\right) +\\frac{y_k}{\\hat{y}_k} \\hat{y}_k (1-\\hat{y}_k)\\right)x_j \\\\\n",
    " &= x_j\\left(-y_k+y_k\\hat{y_k}+\\sum_{k=1, k \\neq j}^K y_k \\hat{y}_j\\right)\\\\\n",
    " &= x_j\\left(\\hat{y_k}\\left(\\sum_{k=1}^K y_k\\right) - y_k\\right)\\\\\n",
    " &= x_j(\\hat{y}_k-y_k)\\\\\n",
    " &= -x_j(y_k-\\hat{y}_k)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)\n",
    "![](task2b_binary_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2b_binary_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "I ended up with early stopping at epoch number 57. However, for the plots, I have chosen to show it with the full number of training steps at 50 epochs so it's easier to see patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)\n",
    "I think we get a large spike every time we introduce a new batch. With no shuffling, every new batch has never-before-seen images. If we shuffle the dataset before we draw a batch from it, we might end up with some images that we actually have seen before too. This is why we have a difference in the spikes, because with shuffling we will see pictures several times.\n",
    "![](task2e_train_accuracy_shuffle_difference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "![](task3b_softmax_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "![](task3b_softmax_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "The validation accuracy seems to generally improve over the training steps. If it were overfit, the validation accuracy would start decreasing again as the training steps progressed. I'm not seeing that here, so I think it's well fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial J}{\\partial \\omega} &=\\frac{\\partial}{\\partial \\omega}(C(\\omega)+\\lambda R(\\omega)) \\\\\n",
    "&=\\frac{\\partial C(\\omega)}{\\partial \\omega}+\\lambda \\frac{\\partial}{\\partial \\omega}\\left(\\frac{1}{2} \\sum_{i, j} \\omega_{i, j}^{2}\\right) \\\\\n",
    "&=-x_{j}^{n}\\left(y_{k}^{n}-\\hat{y}_{k}^{n}\\right)+\\lambda\\left(\\frac{1}{2} \\sum_{i, j} \\frac{\\partial \\omega_{i, j}^{2}}{\\partial \\omega}\\right) \\\\\n",
    "&=-x_{j}^{n}\\left(y_{k}^{n}-\\hat{y}_{k}^{n}\\right)+\\lambda \\sum_{i, j} \\omega_{i, j}\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "$L_2$ regularization puts a limit on how large the weights can be. \n",
    "In the following figure, the top row is $\\lambda=0.0$ and the bottom row is $\\lambda=2.0$\n",
    "\n",
    "I think the weights for the model with $\\lambda=2.0$ are less noisy because the neural network is also forced to choose if increasing the weight for the current pixel is \"worth it\". This means that we do not get the white specks we find in the top row picture, as large values (bright whites) are penalized hard with $\\lambda=2.0$. In other words, I think the neural network has to choose which pixels really are the most important ones. It also seems like the shapes of the numbers are more clearly defined for $\\lambda=2.0$.\n",
    "![](task4b_softmax_weight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "![](task4c_l2_reg_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "In a way, regularization hinders the neural network from obtaining the optimal weights on the dataset. By introducing $L_2$ regularization, we are going from telling the neural network to \"find the best weights given the training set\", to telling the neural network \"find the best weights given the training set, but I do not want the weights to be very large\". This balancing of priorities (which is tuned by $\\lambda$) means that we have to trade off some validation accuracy and in return we (hopefully) get a neural network that is able to handle more general cases (we prevent overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "We see that as $\\lambda$ increases, the $L_2$ norm decreases. This is as expected, because an increase in $\\lambda$ means that we penalize harder the weights ($w$) distance from the norm. Similarly, a low $\\lambda$ means that \"I do not care that the weights are very large, just find the best solution for the training set\", and as such, the weights will be as large as needed to minimize the error on the training data.\n",
    "![](task4d_l2_reg_norms.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
